{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Flatten, Dense, Reshape\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle as cPickle\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score,confusion_matrix,multilabel_confusion_matrix,recall_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import svm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model = 'patch_based_2048_512_128_8.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building MLP model...\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "first (Reshape)              (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "second (Dense)               (None, 2048)              6293504   \n",
      "_________________________________________________________________\n",
      "3 (Dense)                    (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "4 (Dense)                    (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 7,409,288\n",
      "Trainable params: 7,409,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#user defined variables\n",
    "version='imgsize64_batchsize64_4layers_2048_512_128'\n",
    "IMG_SIZE   = 32 #deafult 32\n",
    "BATCH_SIZE  = 64 #default 16\n",
    "DATASET_DIR = '../../../Databases/MIT_split'#'/home/mcv/datasets/MIT_split'\n",
    "MODEL_FNAME = 'output_results/'+fname_model\n",
    "\n",
    "# if not os.path.exists(DATASET_DIR):\n",
    "#   print(Color.RED, 'ERROR: dataset directory '+DATASET_DIR+' does not exist!\\n')\n",
    "#   quit()\n",
    "\n",
    "\n",
    "print('Building MLP model...\\n')\n",
    "\n",
    "#Build the Multi Layer Perceptron model\n",
    "\n",
    "####\n",
    "# Split into tiles\n",
    "####\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((IMG_SIZE*IMG_SIZE*3,),input_shape=(IMG_SIZE, IMG_SIZE, 3),name='first'))\n",
    "model.add(Dense(units=2048, activation='relu',name='second'))\n",
    "model.add(Dense(units=512, activation='relu', name='3'))\n",
    "model.add(Dense(units=128, activation='relu', name='4'))\n",
    "model.add(Dense(units=8, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file=f'output_results/feature_extraction_{version}.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "model.load_weights(MODEL_FNAME)\n",
    "#plot_model(model, to_file=f'output_results/feature_extraction_{version}.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "print('Done!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer = Model(inputs=model.input, outputs=model.get_layer('second').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for class_folder in os.listdir(folder):\n",
    "        for filename in os.listdir(folder+'/'+class_folder):\n",
    "            #print()\n",
    "            img = Image.open(os.path.join(folder+'/'+class_folder,filename))\n",
    "            if img is not None:\n",
    "                \n",
    "                images.append(img)\n",
    "            Image.close(os.path.join(folder+'/'+class_folder,filename))\n",
    "\n",
    "\n",
    "    return images\n",
    "'''\n",
    "def load_images_from_folder(folder):\n",
    "    images=[]\n",
    "    for path, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            \n",
    "            img = Image.open(path+'/'+file) \n",
    "            if img is not None:\n",
    "                \n",
    "                images.append(img.copy())\n",
    "            img.close()\n",
    "    return images\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_folder = '../mcv/datasets/Patches_P32_B64_E50/train'\n",
    "test_images_folder = '../mcv/datasets/Patches_P32_B64_E50/test'\n",
    "train_labels = cPickle.load(open('../mcv/datasets/train_labels.dat','rb'))\n",
    "test_labels = cPickle.load(open('../mcv/datasets/test_labels.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = load_images_from_folder(train_images_folder)\n",
    "test_images = load_images_from_folder(test_images_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "n_patches = int(((256)/32)**2)\n",
    "print(n_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Antoni/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TRAIN ##\n",
    "patch_idx=0\n",
    "Train_descriptors=[]\n",
    "Train_label_per_descriptor=[]\n",
    "for labels in train_labels:\n",
    "    patches=[]\n",
    "    Train_descriptors_patch=[]\n",
    "    for i in range(n_patches):\n",
    "        patches.append(np.asarray(train_images[i+patch_idx]))\n",
    "        \n",
    "    for patch in patches:    \n",
    "        patch = np.expand_dims(np.resize(patch, (IMG_SIZE, IMG_SIZE, 3)), axis=0)\n",
    "        Train_descriptors_patch.append(model_layer.predict(patch/255.0))\n",
    "        \n",
    "    Train_label_per_descriptor.append(labels)\n",
    "        \n",
    "    Train_descriptors.append(Train_descriptors_patch)\n",
    "    Train_label_per_descriptor.append(labels)\n",
    "    patch_idx+=n_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_descriptors_P32_2048.pkl', 'wb') as f:\n",
    "    cPickle.dump(Train_descriptors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "patch_idx=0\n",
    "Test_descriptors=[]\n",
    "Test_label_per_descriptor=[]\n",
    "\n",
    "\n",
    "for labels in test_labels:\n",
    "    patches=[]\n",
    "    Test_descriptors_patch=[]\n",
    "    for i in range(n_patches):\n",
    "        patches.append(np.asarray(test_images[i+patch_idx]))\n",
    "        \n",
    "    for patch in patches:    \n",
    "        patch = np.expand_dims(np.resize(patch, (IMG_SIZE, IMG_SIZE, 3)), axis=0)\n",
    "        Test_descriptors_patch.append(model_layer.predict(patch/255.0))\n",
    "        \n",
    "    Test_label_per_descriptor.append(labels)\n",
    "        \n",
    "    Test_descriptors.append(Test_descriptors_patch)\n",
    "    Test_label_per_descriptor.append(labels)\n",
    "    patch_idx+=n_patches\n",
    "    \n",
    "with open('test_descriptors_P32_2048.pkl', 'wb') as f:\n",
    "    cPickle.dump(Test_descriptors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear kernel only SVM without codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_descriptors=[i[0] for i in Train_descriptors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_descriptors=[i[0] for i in Test_descriptors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC(max_iter=2000, C=10)\n",
    "lin_clf.fit(Train_descriptors, train_labels)\n",
    "\n",
    "scores = cross_val_score(lin_clf, Test_descriptors, Test_label_per_descriptor, cv=5)\n",
    "accuracy=scores.mean()*100\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel only SVM without codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svc = svm.SVC(kernel='rbf', C=1000, gamma=0.001)\n",
    "rbf_svc.fit(Train_descriptors, train_labels)\n",
    "\n",
    "scores = cross_val_score(rbf_svc, Test_descriptors, test_labels, cv=5)\n",
    "\n",
    "accuracy=scores.mean()*100\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptors_bow = [des for descriptors_patches in Train_descriptors for des in descriptors_patches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=2560, compute_labels=False, init='k-means++',\n",
       "                init_size=None, max_iter=100, max_no_improvement=10,\n",
       "                n_clusters=128, n_init=3, random_state=42,\n",
       "                reassignment_ratio=0.0001, tol=0.0, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptors_bow = [des for descriptors_patches in Train_descriptors for des in descriptors_patches]\n",
    "D=np.vstack(train_descriptors_bow)\n",
    "\n",
    "k=128\n",
    "codebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\n",
    "codebook.fit(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_words=np.zeros((len(Train_descriptors),k*n_patches),dtype=np.float32)\n",
    "for idx,patches in enumerate(Train_descriptors):\n",
    "\n",
    "    image_histograms = []\n",
    "    for id2,image_patch in enumerate(patches):\n",
    "        patch_words = codebook.predict(image_patch)\n",
    "        image_histograms.append(normalize(np.bincount(patch_words,minlength=k).reshape(1,-1)))\n",
    "        # concatenated histogram has k(1+2**(2pyramid_levels)) bins\n",
    "        # normalize(np.bincount(words,minlength=k).reshape(1,-1))\n",
    "\n",
    "    concatenated_histogram=image_histograms[0].copy()\n",
    "    for patch_idx in range(1,len(image_histograms)):\n",
    "        # concatenate histograms\n",
    "        concatenated_histogram = np.concatenate((concatenated_histogram,image_histograms[patch_idx]))\n",
    "    visual_words[idx,:]=concatenated_histogram.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12292531, -0.10625593, -0.04616341, ..., -0.02306328,\n",
       "        -0.09549959, -0.06111731],\n",
       "       [-0.12292531, -0.10625593, -0.04616341, ..., -0.02306328,\n",
       "        -0.09549959, -0.06111731],\n",
       "       [-0.12292531, -0.10625593, -0.04616341, ..., -0.02306328,\n",
       "        -0.09549959, -0.06111731],\n",
       "       ...,\n",
       "       [-0.12292531, -0.10625593, -0.04616341, ..., -0.02306328,\n",
       "        -0.09549959, -0.06111731],\n",
       "       [-0.12292531, -0.10625593, -0.04616341, ..., -0.02306328,\n",
       "        -0.09549959, -0.06111731],\n",
       "       [-0.12292531, -0.10625593, -0.04616341, ..., -0.02306328,\n",
       "        -0.09549959, -0.06111731]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(visual_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Test_descriptors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.951154052603336\n"
     ]
    }
   ],
   "source": [
    "lin_clf = svm.LinearSVC(max_iter=2000, C=10)\n",
    "lin_clf.fit(visual_words, train_labels)\n",
    "\n",
    "visual_words_test=np.zeros((len(Test_descriptors),k*n_patches),dtype=np.float32)\n",
    "for idx,patches in enumerate(Test_descriptors):\n",
    "\n",
    "    image_histograms = []\n",
    "    for id2,image_patch in enumerate(patches):\n",
    "        patch_words = codebook.predict(image_patch)\n",
    "        image_histograms.append(normalize(np.bincount(patch_words,minlength=k).reshape(1,-1)))\n",
    "        # concatenated histogram has k(1+2**(2pyramid_levels)) bins\n",
    "        # normalize(np.bincount(words,minlength=k).reshape(1,-1))\n",
    "\n",
    "    concatenated_histogram=image_histograms[0].copy()\n",
    "    for patch_idx in range(1,len(image_histograms)):\n",
    "        # concatenate histograms\n",
    "        concatenated_histogram = np.concatenate((concatenated_histogram,image_histograms[patch_idx]))\n",
    "    visual_words_test[idx,:]=concatenated_histogram.flatten()\n",
    "\n",
    "visual_words_test=scaler.transform(visual_words_test)\n",
    "scores = cross_val_score(lin_clf, visual_words_test, test_labels, cv=5)\n",
    "accuracy=scores.mean()*100\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW with RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptors_bow = [des for descriptors_patches in Train_descriptors for des in descriptors_patches]\n",
    "D=np.vstack(train_descriptors_bow)\n",
    "\n",
    "k=128\n",
    "codebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\n",
    "codebook.fit(D)\n",
    "\n",
    "visual_words=np.zeros((len(Train_descriptors),k),dtype=np.float32)\n",
    "for i in range(len(Train_descriptors)):\n",
    "    words=codebook.predict(Train_descriptors[i])\n",
    "    visual_words[i,:]=normalize(np.bincount(words,minlength=k).reshape(1,-1))\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=37,n_jobs=-1,metric='manhattan')\n",
    "# knn.fit(visual_words, train_labels)\n",
    "\n",
    "today = datetime.now()\n",
    "dt_string = today.strftime(\"%H:%M:%S\")\n",
    "print(f\"{dt_string} started doing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(visual_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.621578099838967\n"
     ]
    }
   ],
   "source": [
    "rbf_svc = svm.SVC(kernel='rbf', C=1000, gamma=0.001)\n",
    "rbf_svc.fit(visual_words, train_labels)\n",
    "\n",
    "visual_words_test=np.zeros((len(Test_descriptors),k*n_patches),dtype=np.float32)\n",
    "for idx,patches in enumerate(Test_descriptors):\n",
    "\n",
    "    image_histograms = []\n",
    "    for id2,image_patch in enumerate(patches):\n",
    "        patch_words = codebook.predict(image_patch)\n",
    "        image_histograms.append(normalize(np.bincount(patch_words,minlength=k).reshape(1,-1)))\n",
    "        # concatenated histogram has k(1+2**(2pyramid_levels)) bins\n",
    "        # normalize(np.bincount(words,minlength=k).reshape(1,-1))\n",
    "\n",
    "    concatenated_histogram=image_histograms[0].copy()\n",
    "    for patch_idx in range(1,len(image_histograms)):\n",
    "        # concatenate histograms\n",
    "        concatenated_histogram = np.concatenate((concatenated_histogram,image_histograms[patch_idx]))\n",
    "    visual_words_test[idx,:]=concatenated_histogram.flatten()\n",
    "\n",
    "visual_words_test=scaler.transform(visual_words_test)\n",
    "scores = cross_val_score(rbf_svc, visual_words_test, test_labels, cv=5)\n",
    "accuracy=scores.mean()*100\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
